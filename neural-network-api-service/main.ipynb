{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# PyTorch Training and FastAPI Deployment Notebook\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "- Load and preprocess a CSV dataset\n",
    "- Split the dataset into training, validation, and test sets\n",
    "- Define a custom PyTorch Dataset and a more complex neural network model (with dropout and batch normalization)\n",
    "- Train the model and save it (reporting metrics including true positive rate and true negative rate)\n",
    "- Write a FastAPI inference endpoint that loads the saved model and provides predictions\n",
    "\n",
    "After running the notebook, you can run the FastAPI server by executing:\n",
    "```bash\n",
    "uvicorn app:app --reload\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('Libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess Data\n",
    "\n",
    "Assume that `data.csv` is in the current directory. We will:\n",
    "\n",
    "- Convert the `Type` column to numeric (filling missing values with 0)\n",
    "- Map the `Label` column so that `Success` becomes 1 and `Fail` becomes 0\n",
    "- Use all columns except `BlockId` and `Label` as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-preprocess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (575061, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = 'HDFS_v1/preprocessed/event_occurrence_matrix.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocess the data\n",
    "df['Type'] = pd.to_numeric(df['Type'], errors='coerce').fillna(0)\n",
    "df['Label'] = df['Label'].apply(lambda x: 1 if x.strip().lower() == 'success' else 0)\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = [col for col in df.columns if col not in ['BlockId', 'Label']]\n",
    "target_col = 'Label'\n",
    "\n",
    "print('Data shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-data",
   "metadata": {},
   "source": [
    "## Step 2: Split the Data and Save as CSVs\n",
    "\n",
    "We split the dataset into training (70%), validation (15%), and test (15%) sets and save them as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "split-save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 402542, Validation samples: 86259, Test samples: 86260\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "df_train, df_temp = train_test_split(df, test_size=0.3, random_state=42, stratify=df[target_col])\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp[target_col])\n",
    "\n",
    "# Save splits as CSV\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_val.to_csv('val.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "\n",
    "print(f\"Train samples: {len(df_train)}, Validation samples: {len(df_val)}, Test samples: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "define-model",
   "metadata": {},
   "source": [
    "## Step 3: Define a Custom Dataset and Neural Network Model\n",
    "\n",
    "Below, we define a custom `CSVDataset` and a more complex neural network model that uses four hidden layers with batch normalization and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dataset-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset and updated model classes defined.\n"
     ]
    }
   ],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, feature_cols, target_col):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.features = self.data[feature_cols].values.astype(np.float32)\n",
    "        self.labels = self.data[target_col].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx])\n",
    "        y = torch.tensor(self.labels[idx])\n",
    "        return x, y\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Third hidden layer\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Fourth hidden layer\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "print('Custom Dataset and updated model classes defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-model",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model and Save It\n",
    "\n",
    "This cell trains the updated model for a few epochs and prints the training loss, validation loss, validation accuracy, true positive rate (TPR), and true negative rate (TNR). Finally, the trained model is saved to `model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "train-loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Train Loss: 0.1197, Val Loss: 0.0096, Val Acc: 99.68%, TPR: 99.92%, TNR: 91.49%\n",
      "Epoch [2/5] - Train Loss: 0.0040, Val Loss: 0.0018, Val Acc: 99.98%, TPR: 99.99%, TNR: 99.68%\n",
      "Epoch [3/5] - Train Loss: 0.0013, Val Loss: 0.0011, Val Acc: 99.98%, TPR: 99.98%, TNR: 99.80%\n",
      "Epoch [4/5] - Train Loss: 0.0010, Val Loss: 0.0010, Val Acc: 99.98%, TPR: 99.98%, TNR: 99.80%\n",
      "Epoch [5/5] - Train Loss: 0.0010, Val Loss: 0.0011, Val Acc: 99.99%, TPR: 99.99%, TNR: 99.80%\n",
      "Model saved as 'model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2048\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "input_dim = len(feature_cols)  # Number of features\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CSVDataset('train.csv', feature_cols, target_col)\n",
    "val_dataset = CSVDataset('val.csv', feature_cols, target_col)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = Net(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Expects raw logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features).squeeze(1)  # Squeeze to match labels shape\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * features.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP = TN = FP = FN = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            outputs = model(features).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * features.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            TP += ((preds == 1) & (labels == 1)).sum().item()\n",
    "            TN += ((preds == 0) & (labels == 0)).sum().item()\n",
    "            FP += ((preds == 1) & (labels == 0)).sum().item()\n",
    "            FN += ((preds == 0) & (labels == 1)).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = correct / total * 100.0\n",
    "    TPR = (TP / (TP + FN) * 100.0) if (TP + FN) > 0 else 0.0\n",
    "    TNR = (TN / (TN + FP) * 100.0) if (TN + FP) > 0 else 0.0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, TPR: {TPR:.2f}%, TNR: {TNR:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print(\"Model saved as 'model.pth'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fastapi-endpoint",
   "metadata": {},
   "source": [
    "## Step 5: Create a FastAPI Inference Endpoint\n",
    "\n",
    "The cell below writes a FastAPI application to a file named `app.py`. This app loads the saved model and exposes a `/predict` endpoint.\n",
    "\n",
    "To run the FastAPI server, execute the following in your terminal:\n",
    "```bash\n",
    "uvicorn app:app --reload\n",
    "```\n",
    "\n",
    "You can then send a POST request to `http://127.0.0.1:8000/predict` with a JSON body containing a list of feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "write-app",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the same deep neural network architecture with dropout\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Set the input dimension (ensure this matches the number of features in your dataset)\n",
    "input_dim = 30\n",
    "\n",
    "# Load the saved model\n",
    "model = Net(input_dim)\n",
    "model.load_state_dict(torch.load('model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "class PredictionInput(BaseModel):\n",
    "    features: list[float]  # List of feature values\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(input_data: PredictionInput):\n",
    "    # Convert the input list into a tensor and add a batch dimension\n",
    "    features_tensor = torch.tensor(input_data.features, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(features_tensor).squeeze(1)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "        predicted_class = 1 if probability > 0.5 else 0\n",
    "    return {\"probability\": probability, \"predicted_class\": predicted_class}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "print('Writing app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-notes",
   "metadata": {},
   "source": [
    "The FastAPI app has now been written to `app.py`. To run the server, open a terminal in the notebook's directory and run:\n",
    "```bash\n",
    "uvicorn app:app --reload\n",
    "```\n",
    "\n",
    "You can then send a POST request to `http://127.0.0.1:8000/predict` with a JSON body containing a list of feature values. For example:\n",
    "\n",
    "```bash\n",
    "curl -X POST \"http://127.0.0.1:8000/predict\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"features\": [0, 3, 1, 15, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 0, 0, 3, 0, 0, 0, 0]}'\n",
    "```\n",
    "\n",
    "Make sure that the length of the `features` list exactly matches the `input_dim` (in this example, 30)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
